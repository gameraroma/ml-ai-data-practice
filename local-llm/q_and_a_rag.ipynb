{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQFo8E6DrH14"
      },
      "source": [
        "# Q&A with Rag using Local Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp6U3FdWrH16"
      },
      "source": [
        "### Document Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNYh54R7rH16"
      },
      "source": [
        "First, install packages needed for local embeddings and vector storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVjnrIKmrH16"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-community langchainhub gpt4all langchain-chroma pypdf langchain-google-vertexai google-cloud-storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LY6hTkjrH18"
      },
      "source": [
        "### Restart current runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rmKb1T8rH18",
        "outputId": "ca48c8ea-5c7c-4a21-9599-cadb9da7c761"
      },
      "outputs": [],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWPch05eTkpc"
      },
      "source": [
        "### Indexing: Load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCoc3yTprH19"
      },
      "source": [
        "Use PyPDFLoader as in this [document](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyyBBStJrH19",
        "outputId": "b359ef22-93e7-43e2-9591-06d16ed34dc6"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "file_path = \"/content/sample_data/Owner-Manual-KICKS-e-POWER-EN.pdf\"\n",
        "loader = PyPDFLoader(file_path)\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "504vEuT5UKYA"
      },
      "source": [
        "### Indexing: Split\n",
        "\n",
        "To handle this we’ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the manual at run time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UtQUuQcxe1c"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
        ")\n",
        "all_splits = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_yxin5exo1z",
        "outputId": "3e0b746f-bce0-41cc-eee0-f8a900316668"
      },
      "outputs": [],
      "source": [
        "len(all_splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRjZPLc7rH19"
      },
      "source": [
        "Next, the below steps will download the GPT4All embeddings locally (if you don't already have them)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjrBVVEqx0vn"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "\n",
        "model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
        "gpt4all_kwargs = {'allow_download': 'True'}\n",
        "embeddings = GPT4AllEmbeddings(\n",
        "    model_name=model_name,\n",
        "    gpt4all_kwargs=gpt4all_kwargs\n",
        ")\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMML12r-0dYi"
      },
      "source": [
        "### Retrieval and Generation: Retrieve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tohDCn1R02Uo"
      },
      "source": [
        "Now let’s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-3gdwqF04E-",
        "outputId": "7c15539c-2435-4ad6-e933-41476c6d9e26"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
        "retrieved_docs = retriever.invoke(\"What is auto brake hold?\")\n",
        "len(retrieved_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_zT_Cu21LXz"
      },
      "outputs": [],
      "source": [
        "print(retrieved_docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PYafhEj4yjN"
      },
      "source": [
        "### Retrieval and Generation: Generate\n",
        "Let’s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz6yty9j46ED"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ijive62YYV_"
      },
      "outputs": [],
      "source": [
        "# Authenticate\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ybjQldab4Lv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV8I72305v_D"
      },
      "source": [
        "Set environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7Cgwyhh4yU4",
        "outputId": "a0e1b4a1-ebb2-4edb-911a-302461e48a46"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kfSmOE35RzA",
        "outputId": "6cfbcebc-432b-4d7b-d6cd-f8b9bbc7f689"
      },
      "outputs": [],
      "source": [
        "from langchain_google_vertexai import ChatVertexAI\n",
        "\n",
        "llm = ChatVertexAI(model=\"gemini-1.5-pro\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT7s565WWJwJ"
      },
      "source": [
        "Let’s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TebqjHGyRgiB",
        "outputId": "4966bf4e-8a67-4be2-a810-14941b51022f"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "custom_rag_prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | custom_rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"What is auto brake hold and how does function be?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
