{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQFo8E6DrH14"
      },
      "source": [
        "# Q&A with Rag using Local Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prerequisite\n",
        "\n",
        "You need to download and store 2 GGUF files in the same folder as this notebook file\n",
        "1. Meta-Llama-3.1-8B-Instruct-Q3_K_L.gguf\n",
        "1. nomic-embed-text-v1.5.Q2_K.gguf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp6U3FdWrH16"
      },
      "source": [
        "### Document Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNYh54R7rH16"
      },
      "source": [
        "First, install packages needed for local embeddings and vector storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVjnrIKmrH16"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet langchain langchain-community langchainhub gpt4all langchain-chroma pypdf gpt4all nomic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LY6hTkjrH18"
      },
      "source": [
        "### Restart current runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rmKb1T8rH18",
        "outputId": "ca48c8ea-5c7c-4a21-9599-cadb9da7c761"
      },
      "outputs": [],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWPch05eTkpc"
      },
      "source": [
        "### Indexing: Load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCoc3yTprH19"
      },
      "source": [
        "Use PyPDFLoader as in this [document](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyyBBStJrH19",
        "outputId": "b359ef22-93e7-43e2-9591-06d16ed34dc6"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "file_path = \"../data/Owner-Manual-KICKS-e-POWER-EN.pdf\"\n",
        "loader = PyPDFLoader(file_path)\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "504vEuT5UKYA"
      },
      "source": [
        "### Indexing: Split\n",
        "\n",
        "To handle this we’ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the manual at run time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UtQUuQcxe1c"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
        ")\n",
        "all_splits = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_yxin5exo1z",
        "outputId": "3e0b746f-bce0-41cc-eee0-f8a900316668"
      },
      "outputs": [],
      "source": [
        "type(all_splits[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRjZPLc7rH19"
      },
      "source": [
        "Next, the below steps will download the GPT4All embeddings locally (if you don't already have them)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjrBVVEqx0vn"
      },
      "outputs": [],
      "source": [
        "from nomic import embed\n",
        "\n",
        "texts = [i.page_content for i in all_splits]\n",
        "embeddings = embed.text(texts, inference_mode=\"local\")['embeddings']\n",
        "print(\"Number of embeddings created:\", len(embeddings))\n",
        "print(\"Number of dimensions per embedding:\", len(embeddings[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load to vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import GPT4AllEmbeddings\n",
        "\n",
        "model_path = \"./all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
        "gpt4all_kwargs = {'allow_download': 'false'}\n",
        "embeddings = GPT4AllEmbeddings(\n",
        "    model_path=model_path,\n",
        "    gpt4all_kwargs=gpt4all_kwargs\n",
        ")\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMML12r-0dYi"
      },
      "source": [
        "### Retrieval and Generation: Retrieve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tohDCn1R02Uo"
      },
      "source": [
        "Now let’s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-3gdwqF04E-",
        "outputId": "7c15539c-2435-4ad6-e933-41476c6d9e26"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
        "retrieved_docs = retriever.invoke(\"What is auto brake hold?\")\n",
        "len(retrieved_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_zT_Cu21LXz"
      },
      "outputs": [],
      "source": [
        "print(retrieved_docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PYafhEj4yjN"
      },
      "source": [
        "### Retrieval and Generation: Generate\n",
        "Let’s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz6yty9j46ED"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV8I72305v_D"
      },
      "source": [
        "Set environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kfSmOE35RzA",
        "outputId": "6cfbcebc-432b-4d7b-d6cd-f8b9bbc7f689"
      },
      "outputs": [],
      "source": [
        "from langchain_community.llms.gpt4all import GPT4All\n",
        "\n",
        "llm = GPT4All(model=('./Meta-Llama-3.1-8B-Instruct-Q3_K_L.gguf'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT7s565WWJwJ"
      },
      "source": [
        "Let’s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TebqjHGyRgiB",
        "outputId": "4966bf4e-8a67-4be2-a810-14941b51022f"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "custom_rag_prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | custom_rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = rag_chain.invoke(\"What is auto brake hold and how does function be?\")\n",
        "print(f\"{res}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > The automatic brake hold (ABH) system maintains braking force without driver having to depress the brake pedal when vehicle stops at a traffic light or intersection. It can be activated by pressing the ABH switch while driving, but it will reset off every time power is switched from \"OFF\" position to \"ON\". To use the function, driver's seatbelt must be fastened and electronic parking brake released. The system helps maintain steering control and minimizes swerving and spinning on slippery surfaces.\n",
        ">\n",
        "> The automatic brake hold indicator light illuminates when ABH becomes standby (white) or active (green). When activated, it maintains braking force until accelerator pedal is pressed again to deactivate the function. If a malfunction occurs in the ABH function, an error message may appear in the vehicle information display. The system also has warnings and indicators for steep slope conditions.\n",
        ">\n",
        "> The automatic brake hold can be deactivated by pressing the brake pedal firmly while pushing the ABH switch or when driver's seatbelt is unfastened, door opened, power switch placed in \"OFF\" position, parking brake applied manually, or an error occurs in the ABH function. The system prevents wheels from locking during hard braking on slippery surfaces and helps maintain steering control.\n",
        ">\n",
        "> The automatic brake hold indicator light may appear in the vehicle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### References\n",
        "- Langchain Docs\n",
        "- GPT4ALL Python SDK Docs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
